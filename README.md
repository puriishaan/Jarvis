Jarvis is a multi-threaded, multi-modal AI assistant designed to interpret natural language, recognize hand gestures, and control a virtual machine environment in real time. 
At its core, the system is built using Python and leverages multiple daemon threads to handle distinct subsystems concurrently—ensuring responsiveness across speech input, 
gesture detection, text-to-speech output, and command execution. When a user speaks, the system runs a natural language processing loop (run_jarvis_once) that processes 
the input using a large language model (like OpenAI’s GPT), parses the structured JSON response, and routes the textual reply to a speech synthesis module (speak_text) while 
queuing any actionable instructions for the Orgo VM controller. The system also includes a vision-based hand gesture detection module, which uses a webcam feed processed 
with MediaPipe to extract hand landmarks. These landmarks are classified using a pre-trained machine learning model (loaded via joblib), and high-confidence gestures are 
mapped to system commands like "swipe left" or "screenshot," then queued for execution. All commands, whether generated by voice or gestures, are dispatched to the Orgo 
virtual machine through an abstraction layer (send_prompt_to_computer), which issues prompt-based actions such as opening applications or navigating interfaces. Additionally, 
a GUI runs in a dedicated thread—likely built with Tkinter or PyQt—providing an interactive interface for visual feedback or input. Altogether, this architecture allows Jarvis 
to serve as an intelligent, voice- and gesture-driven agent capable of interfacing with digital systems in a natural and intuitive manner.



Created by Ishaan Puri




